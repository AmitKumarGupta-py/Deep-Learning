{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    " \n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "inpDir = '../input' # Input Stored here\n",
    "outDir = '../output' # output Here\n",
    "modelDir = './models'# to save Models\n",
    "subDir = 'checkpoints' # sub dir by dataset\n",
    "RANDOM_STATE = 24\n",
    "np.random.RandomState(seed = RANDOM_STATE)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NOISE = 0.2 \n",
    "ALPHA = 0.001 \n",
    "EPOCHS = 45\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the plotting parameters\n",
    "params = {\n",
    "    'legend.fontsize': 'medium',\n",
    "    'figure.figsize':(15,6),\n",
    "    'axes.labelsize':'medium',\n",
    "    'axes.titlesize':'medium',\n",
    "    'xtick.labelsize': 'medium',\n",
    "    'ytick.labelsize':'medium',\n",
    "    #'text.usetex':True,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "CMAP = plt.cm.coolwarm\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{\n",
    "    Load Weather Data\n",
    "}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input\\\\text_gen\\\\shakespeare.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m file_loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_gen\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mshakespeare.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(inpDir,file_loc)\n\u001b[1;32m----> 4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m text[:\u001b[38;5;241m100\u001b[39m]\n",
      "File \u001b[1;32m~.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input\\\\text_gen\\\\shakespeare.txt'"
     ]
    }
   ],
   "source": [
    "file_loc = 'text_gen\\shakespeare.txt'\n",
    "file_path = os.path.join(inpDir,file_loc)\n",
    "\n",
    "text = open(file_path, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text {Character Based Modelling}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "example_per_epoch = len(text) / seq_len+1\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(20):\n",
    "    print(i.numpy(), end=' : ')\n",
    "    print(idx2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(2):\n",
    "    print(item)\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_intput_target(chunk):\n",
    "    input_text = chunk[:-1]  # First hundred charater\n",
    "    target_text = chunk[1:]  # Last character\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "dataset = sequences.map(split_intput_target)\n",
    "\n",
    "for inp_epx, tar_epx in dataset.take(2):\n",
    "    print(repr(''.join(idx2char[inp_epx.numpy()])))\n",
    "    print(repr(''.join(idx2char[tar_epx.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFRE_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFRE_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size=BATCH_SIZE):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(None,), batch_size=batch_size),\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            recurrent_initializer='orthogonal',\n",
    "        ),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Testing, Check for models health withount trianing\n",
    "for inp_ex, tar_ex in dataset.take(1):\n",
    "    ex_pred = model(inp_ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = tf.random.categorical(ex_pred[0], num_samples=1)\n",
    "print(sample_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = tf.squeeze(sample_indices, axis=-1).numpy()\n",
    "print(sample_indices)\n",
    "print(repr(''.join(idx2char[sample_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = loss_fn,\n",
    "    metrics = [\n",
    "        'accuracy'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point_path = os.path.join(outDir, subDir)\n",
    "check_point_prefix = os.path.join(check_point_path, \"checkpoint-{epoch}.keras\")\n",
    "check_point_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_point_prefix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs =EPOCHS,\n",
    "    callbacks = [check_point_callback],\n",
    "    verbose = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(history.history)\n",
    "loss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min loss\n",
    "loss_df.loc[loss_df['loss'] ==loss_df['loss'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = loss_df.plot()\n",
    "ax.vlines(\n",
    "    loss_df.loc[loss_df['loss'] ==loss_df['loss'].min()].index,\n",
    "    ymin=loss_df.loc[loss_df['loss'] ==loss_df['loss'].min()].values[0][1],\n",
    "    ymax=loss_df.loc[loss_df['loss'] ==loss_df['loss'].max()].values[0][1],\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Best model}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "num = loss_df.loc[loss_df['loss'] ==loss_df['loss'].min()].index.to_numpy()[0]\n",
    "# Fabricate full path\n",
    "check_point_weight_path = os.path.join(check_point_path, f\"checkpoint-{num}.keras\")\n",
    "check_point_weight_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initintalize new model\n",
    "model1 = build_model(\n",
    "    vocab_size=vocab_size, \n",
    "    embedding_dim=embedding_dim, \n",
    "    rnn_units=rnn_units, \n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "model1.load_weights(check_point_weight_path)\n",
    "model1.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_sting):\n",
    "    num_generate = 1000\n",
    "    # Getting indices for start string\n",
    "    input_eval = [char2idx[s] for s in start_sting] # List construct\n",
    "    print(input_eval)\n",
    "    input_eval = tf.expand_dims(input_eval, 0) # Size [1,1,5]\n",
    "    print(input_eval)\n",
    "    # store generated text\n",
    "    generated_text = []\n",
    "    for i in range(num_generate):\n",
    "        prediction = model(input_eval)\n",
    "        prediction = tf.squeeze(prediction,0)\n",
    "        # this is where rummer meets the road\n",
    "        prediction_td = tf.random.categorical(prediction, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([prediction_td],0)\n",
    "        generated_text.append(idx2char[prediction_td])\n",
    "    return start_sting + ''.join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model=model1, start_sting='ROMEO:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
