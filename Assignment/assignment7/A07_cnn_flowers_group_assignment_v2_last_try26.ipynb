{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "679Lmwt3l1Bk"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5.tar.gz (10.7 MB)\n",
      "     ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/10.7 MB 682.7 kB/s eta 0:00:16\n",
      "     ---------------------------------------- 0.1/10.7 MB 1.3 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.5/10.7 MB 4.8 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.2/10.7 MB 7.4 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.1/10.7 MB 10.3 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 3.7/10.7 MB 14.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 5.8/10.7 MB 18.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 8.4/10.7 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.7/10.7 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.7/10.7 MB 31.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~harset-normalizer (C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [33 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 112, in get_requires_for_build_wheel\n",
      "      backend = _build_backend()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
      "      obj = import_module(mod_path)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "      return _bootstrap._gcd_import(name[level:], package, level)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "    File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "    File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "    File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "    File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "    File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "    File \"C:\\Users\\dai\\AppData\\Local\\Temp\\pip-build-env-fw4m1gyx\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "      import setuptools.version\n",
      "    File \"C:\\Users\\dai\\AppData\\Local\\Temp\\pip-build-env-fw4m1gyx\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "      import pkg_resources\n",
      "    File \"C:\\Users\\dai\\AppData\\Local\\Temp\\pip-build-env-fw4m1gyx\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "      register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.23.5 scipy --upgrade scikit-learn --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOT-pQNNUAhA"
   },
   "source": [
    "Names and PRN for all group members:\n",
    "- Name: Amit Kumar Gupta\n",
    "- PRN:  240840128006\n",
    "- Date: 6Dec,2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTX3d6qEYhvg"
   },
   "source": [
    "# Deep Neural Networks\n",
    "## A07: Group Assignment\n",
    "\n",
    "\n",
    "##  Convolutional Neural Network (CNN)\n",
    "\n",
    "- Flowers Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iAve6DCL4JH4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\__init__.py:145: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 2.1.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\dai\\AppData\\Local\\Temp\\ipykernel_36836\\573303214.py\", line 8, in <module>\n",
      "    from sklearn import datasets\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 87, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils import _IS_32BIT\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 16, in <module>\n",
      "    from scipy.sparse import issparse\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py\", line 274, in <module>\n",
      "    from ._csr import *\n",
      "  File \"C:\\Users\\dai.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m###-----------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m### Import Libraries\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m###-----------------\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env pyhton3\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# -*- coding\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m###-----------------\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m### Import Libraries\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m###-----------------\u001b[39;00m\n",
      "File \u001b[1;32m~.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py:87\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     85\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     )\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     90\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    134\u001b[0m     ]\n",
      "File \u001b[1;32m~.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n",
      "File \u001b[1;32m~.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compress, islice\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n",
      "File \u001b[1;32m~.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:274\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~.STUDENTSDC\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix, _array_doc_to_matrix\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "\n",
    "#!/usr/bin/env pyhton3\n",
    "# -*- coding\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-56YHubYhvq"
   },
   "outputs": [],
   "source": [
    "###----------------------\n",
    "### Some basic parameters\n",
    "###----------------------\n",
    "# Global variables\n",
    "inpDir = './input' # Input Stored here\n",
    "outDir = './ouput' # output Here\n",
    "modelDir = './models'# to save Models\n",
    "subDir = 'flower_photos' # sub dir by dataset\n",
    "altName = 'base_model'\n",
    "RANDOM_STATE = 24\n",
    "np.random.RandomState(seed = RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "#rng = np.random.default_rng(seed = RANDOM_STATE)\n",
    "#N_SAMPLE = 1000\n",
    "TEST_SIZE = 0.2 # 18147 - 56*256\n",
    "ALPHA = 0.001 # learning rate\n",
    "NOISE = 0.2 # Error\n",
    "EPOCHS = 41\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "PATIENCE = 20\n",
    "LR_FACTOR  =0.8\n",
    "LR_PATIENCE = 10\n",
    "IMG_HEIGHT = 188\n",
    "IMG_WIDTH = 188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezzcIskdUAhF"
   },
   "outputs": [],
   "source": [
    "#set the plotting parameters\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 8),\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'large',\n",
    "          'ytick.labelsize':'large'\n",
    "         }\n",
    "    #'text.usetex':True,\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "CMAP = plt.cm.coolwarm\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ9KD8ZnYhv0"
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "Images are under flower_photos\n",
    "\n",
    "     |- daisy\n",
    "     |- dandelion\n",
    "     |- roses\n",
    "     |- sunflowers\n",
    "     |- tulips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FxqsV2_Yhv1"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    For reading from web link\n",
    "'''\n",
    "\n",
    "# import pathlib\n",
    "# dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "# data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "#                                   fname='flower_photos',\n",
    "#                                   untar=True)\n",
    "# data_dir = pathlib.Path(data_dir)\n",
    "'''\n",
    "    For reading from local directory\n",
    "'''\n",
    " data_dir = os.path.join(inpDir, subDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQTLB3cPUAhG"
   },
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwS4hFUFUAhG"
   },
   "outputs": [],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6o6SPnOUOHg"
   },
   "outputs": [],
   "source": [
    "def fn_plot_tf_hist(hist_df):\n",
    "\n",
    "    '''\n",
    "    Args:\n",
    "        hist_df: a dataframe with following ccolumns\n",
    "            column 0: accuracy\n",
    "            column 1: loss\n",
    "            column 2: val_accuracy\n",
    "            column 3: val_loss\n",
    "            While plotting columns are accessed by index\n",
    "            so that even if the column names are different it will not throw exceptions.\n",
    "    '''\n",
    "\n",
    "    fig, axes = plt.subplots(1,2 , figsize = (15,6))\n",
    "\n",
    "    # properties  matplotlib.patch.Patch\n",
    "    props = dict(boxstyle='round', facecolor='aqua', alpha=0.4)\n",
    "    facecolor = 'cyan'\n",
    "    fontsize=12\n",
    "    CMAP = plt.cm.coolwarm\n",
    "\n",
    "    # Get columns by index to eliminate any column naming error\n",
    "    y1 = hist_df.columns[0]\n",
    "    y2 = hist_df.columns[1]\n",
    "    y3 = hist_df.columns[2]\n",
    "    y4 = hist_df.columns[3]\n",
    "\n",
    "    # Where was min loss\n",
    "    best = hist_df[hist_df[y4] == hist_df[y4].min()]\n",
    "\n",
    "    ax = axes[0]\n",
    "\n",
    "    hist_df.plot(y = [y2,y4], ax = ax, colormap=CMAP)\n",
    "\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = '{:s}: \\n  train: {:6.4f}\\n   test: {:6.4f}'\n",
    "    txtstr = txtFmt.format(y2.capitalize(),\n",
    "                           hist_df.iloc[-1][y2],\n",
    "                           hist_df.iloc[-1][y4]) #text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.95, txtstr, transform=ax.transAxes, fontsize=fontsize,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # calculate offset for arroe\n",
    "    y_min = min(hist_df[y2].min(), hist_df[y4].min())\n",
    "    y_max = max(hist_df[y2].max(), hist_df[y4].max())\n",
    "    offset = (y_max-y_min)/10.0\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Min: {best[y4].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best.index.to_numpy(), best[y4].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best.index.to_numpy(), best[y4].to_numpy()[0] + offset), # location of text\n",
    "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
    "\n",
    "    # Draw vertical line at best value\n",
    "    ax.axvline(x = best.index.to_numpy(), color = 'green', linestyle='-.', lw = 3)\n",
    "\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(y2.capitalize())\n",
    "    ax.set_title('Errors')\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc = 'upper left') # model legend to upper left\n",
    "\n",
    "    ax = axes[1]\n",
    "\n",
    "    hist_df.plot( y = [y1, y3], ax = ax, colormap=CMAP)\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = '{:s}: \\n  train: {:6.4f}\\n  test:  {:6.4f}'\n",
    "    txtstr = txtFmt.format(y1.capitalize(),\n",
    "                           hist_df.iloc[-1][y1],\n",
    "                           hist_df.iloc[-1][y3]) #text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.2, txtstr, transform=ax.transAxes, fontsize=fontsize,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # calculate offset for arroe\n",
    "    y_min = min(hist_df[y1].min(), hist_df[y3].min())\n",
    "    y_max = max(hist_df[y1].max(), hist_df[y3].max())\n",
    "    offset = (y_max-y_min)/10.0\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Best: {best[y3].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best.index.to_numpy(), best[y3].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best.index.to_numpy(), best[y3].to_numpy()[0]-offset), # location of text\n",
    "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
    "\n",
    "\n",
    "    # Draw vertical line at best value\n",
    "    ax.axvline(x = best.index.to_numpy(), color = 'green', linestyle='-.', lw = 3)\n",
    "\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(y1.capitalize())\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc = 'lower left')\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzilHxHgUAhH"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "     data_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "    subset='training',\n",
    "    seed = RANDOM_STATE,\n",
    "    validation_split=TEST_SIZE,\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oah38U8-UAhH"
   },
   "outputs": [],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "     data_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "    subset='validation',\n",
    "    seed = RANDOM_STATE,\n",
    "    validation_split=TEST_SIZE,\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxcxBKtxUAhH"
   },
   "outputs": [],
   "source": [
    "#Is it getting the class names\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "display(class_names, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRvrZxqYUAhH"
   },
   "outputs": [],
   "source": [
    "# sample DATA plot: Display a grid of images from the dataset along with their labels\n",
    "\n",
    "\n",
    "\n",
    "#create a new figure for the grid of images with a specified size\n",
    "fig = plt.figure(figsize  =(15,12))\n",
    "\n",
    "#Adjust Margins\n",
    "fig.subplots_adjust(left = 0, right = 1, bottom=0, top = 1, hspace =0.05, wspace = 0.05)\n",
    "i= 0\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(BATCH_SIZE):\n",
    "        plt.subplot(BATCH_SIZE// 8,8, i+1)\n",
    "        plt.grid(False)\n",
    "\n",
    "        plt.imshow(images[i].numpy().astype('uint8'), cmap = plt.cm.binary)\n",
    "\n",
    "        # Add the numeric label to the image in blue text at a specific position\n",
    "        plt.title(class_names[labels[i].numpy()])\n",
    "\n",
    "\n",
    "        # Add the class name of the image in red text below the numeric label\n",
    "        plt.text(2,4, labels[i].numpy(), color = 'b', fontsize=16)\n",
    "    #SWITCH OF AXIS\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()\n",
    "#Display the entire frid of images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGFG9bxBUAhI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74o8xbmIU2NK"
   },
   "outputs": [],
   "source": [
    "def fn_plot_label(tr_ds, ts_ds, class_names = None):\n",
    "\n",
    "    '''\n",
    "        Args:\n",
    "            tr_ds :  Training Dataset\n",
    "            ts_ds : Testing dataset\n",
    "            class_names : Class names\n",
    "        Returns : none\n",
    "    '''\n",
    "\n",
    "    # create figure and axes\n",
    "    fig, axes = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "    # get names of the classes\n",
    "    if not class_names:\n",
    "        tr_class_names = tr_ds.class_names\n",
    "        ts_class_names = tr_ds.class_names\n",
    "\n",
    "    # pick first axis\n",
    "    ax = axes[0]\n",
    "\n",
    "    # create dict of training labels\n",
    "    class_counts = {}\n",
    "    for imgs, lbls in tr_ds:\n",
    "        for lbl in lbls.numpy():\n",
    "            class_counts[lbl] = class_counts.get(lbl, 0) +1\n",
    "\n",
    "    # bar plot\n",
    "    ax.bar(tr_class_names, [class_counts.get(i, 0) for i in range(len(tr_class_names))],\n",
    "           align='center',color = 'DarkBlue', alpha = 0.7)\n",
    "\n",
    "    # add title\n",
    "    ax.set_title('Training Set')\n",
    "\n",
    "    # grids make it look good\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "    #pick second image\n",
    "    ax = axes[1]\n",
    "\n",
    "    # create dict of training labels\n",
    "    class_counts = {}\n",
    "    for imgs, lbls in ts_ds:\n",
    "        for lbl in lbls.numpy():\n",
    "            class_counts[lbl] = class_counts.get(lbl, 0) +1\n",
    "\n",
    "    # bar plot\n",
    "    ax.bar(ts_class_names, [class_counts.get(i, 0) for i in range(len(ts_class_names))],\n",
    "           align='center',color = 'orange', alpha = 0.7)\n",
    "\n",
    "    # add title\n",
    "    ax.set_title('Test Set')\n",
    "\n",
    "\n",
    "    # grids make it look good\n",
    "    ax.grid(True)\n",
    "\n",
    "    # fit the subplot(s) in to the figure area\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # display all open figures\n",
    "    plt.show()\n",
    "\n",
    "###-------------------------------\n",
    "### Convert datetime from a string\n",
    "###-------------------------------\n",
    "def fn_convert_timestamp(tstr):\n",
    "    '''\n",
    "        Function to conver string of form \"2015-11-12 1444\"\n",
    "    '''\n",
    "\n",
    "    return datetime.strptime(tstr, \"%Y-%m-%d_%H%M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1PDauuiUAhI"
   },
   "outputs": [],
   "source": [
    "fn_plot_label(train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMqt4itjUAhI"
   },
   "outputs": [],
   "source": [
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNQCFLpSUAhI"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeupCGH7UAhI"
   },
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.RandomZoom(height_factor=(-.2, -.2),\n",
    "                                 width_factor=(-.2, -.2),\n",
    "                                  seed = RANDOM_STATE)\n",
    "\n",
    "# layer = tf.keras.layers.RandomRotation(\n",
    "#     (-0.5,0.5),\n",
    "#     fill_mode='nearest',\n",
    "\n",
    "#     seed=RANDOM_STATE,\n",
    "\n",
    "# )\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "img_num = 2\n",
    "for imgs, lbls in train_ds.take(1):\n",
    "    out_image = layer(imgs)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original')\n",
    "    plt.imshow(imgs[img_num].numpy().astype('uint8'))\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Zoomed')\n",
    "    plt.imshow(out_image[img_num].numpy().astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mafLzQVDUAhI"
   },
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.RandomRotation(\n",
    "    (-0.5,0.5),\n",
    "    fill_mode='nearest',\n",
    "\n",
    "    seed=RANDOM_STATE,\n",
    "\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "img_num = 2\n",
    "for imgs, lbls in train_ds.take(1):\n",
    "    out_image = layer(imgs)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original')\n",
    "    plt.imshow(imgs[img_num].numpy().astype('uint8'))\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Zoomed')\n",
    "    plt.imshow(out_image[img_num].numpy().astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA_choS6UAhI"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size = tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFY0x225UAhI"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6tKV1nqUAhJ"
   },
   "source": [
    "* Dropout-> uniform = 0.3, val_loss = val_accuracy ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWEGkI03Ywl7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oteUSODaUAhJ"
   },
   "outputs": [],
   "source": [
    "krnl_initializer =tf.keras.initializers.GlorotUniform(seed = RANDOM_STATE)\n",
    "krnl_reg = None\n",
    "#tf.keras.regularizers.L2(l2=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Dropout\n",
    "dor0 = 0.05\n",
    "dor1 = 0.13\n",
    "dor2 = 0.23\n",
    "dor3 = 0.33\n",
    "dor4 = 0.4\n",
    "dor5 = 0.5\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape = input_shape, name = 'input')\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Rescaling(1./255.)(inputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##....\n",
    "#Set1\n",
    "#conv\n",
    "\n",
    "x = tf.keras.layers.Conv2D(64, (3,3), kernel_initializer=krnl_initializer,\n",
    "\n",
    "                          padding='same',\n",
    "\n",
    "                         name = 'conv_1')(x) #o/p (188,188,64)\n",
    "#batch norm\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "#activation\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "#Dropout layer\n",
    "x = tf.keras.layers.Dropout(dor3)(x)\n",
    "\n",
    "# MAxpool\n",
    "x = tf.keras.layers.MaxPool2D(kernel_size=(2,2), stride=(2, 2), name='mp_1')(x) #o/p (94,94,64)\n",
    "\n",
    "\n",
    "\n",
    "##....\n",
    "#Set2\n",
    "#conv\n",
    "\n",
    "x = tf.keras.layers.Conv2D(128, (3,3), kernel_initializer=krnl_initializer,\n",
    "\n",
    "                         name = 'conv_2')(x) #o/p (92,92,128)\n",
    "#batch norm\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "#activation\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "#Dropout layer\n",
    "x = tf.keras.layers.Dropout(dor3)(x)\n",
    "\n",
    "# MAxpool\n",
    "x = tf.keras.layers.MaxPool2D(kernel_size=(2,2), stride=(2, 2), name='mp_2')(x) #o/p (46,46,128)\n",
    "\n",
    "\n",
    "\n",
    "##....\n",
    "#Set3\n",
    "#conv\n",
    "\n",
    "x = tf.keras.layers.Conv2D(256, (3,3), kernel_initializer=krnl_initializer,\n",
    "\n",
    "                         name = 'conv_3')(x) #o/p (44,44,256)\n",
    "#batch norm\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "#activation\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "#Dropout layer\n",
    "x = tf.keras.layers.Dropout(dor3)(x)\n",
    "\n",
    "# MAxpool\n",
    "x = tf.keras.layers.MaxPool2D(kernel_size=(2,2), stride=(2, 2), name='mp_3')(x) #o/p (22,22,256)\n",
    "\n",
    "\n",
    "\n",
    "##....\n",
    "#Set4\n",
    "#conv\n",
    "\n",
    "x = tf.keras.layers.Conv2D(512, (3,3), kernel_initializer=krnl_initializer,\n",
    "\n",
    "                         name = 'conv_4')(x) #o/p (20,20,512)\n",
    "#batch norm\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "#activation\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "#Dropout layer\n",
    "x = tf.keras.layers.Dropout(dor3)(x)\n",
    "\n",
    "# MAxpool\n",
    "x = tf.keras.layers.MaxPool2D(kernel_size=(2,2), stride=(2, 2), name='mp_4')(x) #o/p (10,10,512)\n",
    "\n",
    "#Set5\n",
    "#conv\n",
    "\n",
    "x = tf.keras.layers.Conv2D(256, (3,3), kernel_initializer=krnl_initializer,\n",
    "\n",
    "                          name = 'conv_5')(x) #o/p (8,8,256)\n",
    "\n",
    "#batch norm\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "#activation\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "#Dropout layer\n",
    "x = tf.keras.layers.Dropout(dor3)(x)\n",
    "\n",
    "\n",
    "\n",
    "# MAxpool\n",
    "x = tf.keras.layers.MaxPool2D(kernel_size=(2,2), stride=(2, 2), name='mp_5')(x) #o/p (4,4,512)\n",
    "\n",
    "\n",
    "# #Set6\n",
    "# #conv\n",
    "\n",
    "x = tf.keras.layers.Conv2D(512, (3,3), kernel_initializer=krnl_initializer,\n",
    "\n",
    "                          name = 'conv_6')(x) #o/p (2,2,1024)\n",
    "\n",
    "#batch norm\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "#activation\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "#Dropout layer\n",
    "x = tf.keras.layers.Dropout(dor3)(x)\n",
    "\n",
    "# #Set7\n",
    "# #conv\n",
    "\n",
    "# x = tf.keras.layers.Conv2D(4096, (1,1), kernel_initializer=krnl_initializer,\n",
    "\n",
    "#                           name = 'conv_7')(x) #o/p (2,2,4096)\n",
    "\n",
    "# #batch norm\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# #activation\n",
    "# x = tf.keras.layers.ReLU()(x)\n",
    "# #Dropout layer\n",
    "# x = tf.keras.layers.Dropout(dor4)(x)\n",
    "\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Dense(4096,\n",
    "                         name = 'fc_1')(x)\n",
    "#batch norm\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "#activation\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "#Dropout layer\n",
    "x = tf.keras.layers.Dropout(dor3)(x)\n",
    "\n",
    "# x = tf.keras.layers.Dense(8192,\n",
    "#                          name = 'fc_2')(x)\n",
    "# #batch norm\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# #activation\n",
    "# x = tf.keras.layers.ReLU()(x)\n",
    "# #Dropout layer\n",
    "# x = tf.keras.layers.Dropout(dor1)(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5,\n",
    "                             activation='softmax', name = 'output')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs = inputs, outputs = outputs, name = 'Flowers')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEK754JZUAhJ"
   },
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = ALPHA)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBj378FWUAhJ"
   },
   "outputs": [],
   "source": [
    "chkptFilePath = os.path.join(modelDir, subDir, f'{altName}.weights.h5')\n",
    "PATIENCE = 30\n",
    "\n",
    "mcb = tf.keras.callbacks.ModelCheckpoint(chkptFilePath,\n",
    "                                        monitor = 'val_loss',\n",
    "                                        verbose = 1,\n",
    "                                        save_best_only = True,\n",
    "                                        save_weights_only = True)\n",
    "\n",
    "escb = tf.keras.callbacks.EarlyStopping(patience = PATIENCE,\n",
    "                                       verbose = 1,\n",
    "                                       restore_best_weights = True)\n",
    "\n",
    "lrcb = tf.keras.callbacks.ReduceLROnPlateau(factor = LR_FACTOR,\n",
    "                                           patience = LR_PATIENCE,\n",
    "                                           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "neVso9tzaYsU",
    "outputId": "0b2ab34a-2c95-4974-bb2d-5e1d2cb62985"
   },
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(train_ds, epochs=EPOCHS,\n",
    "                    callbacks = [mcb, escb, lrcb],\n",
    "                    validation_data=test_ds,\n",
    "                   verbose = 1)\n",
    "\n",
    "# results and inferences\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "display(hist_df.head(3))\n",
    "display(hist_df.tail(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNyyOapRVbvb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CiPjPw8aYsU"
   },
   "outputs": [],
   "source": [
    "fn_plot_tf_hist(hist_df = hist_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCpeLlZUaYsU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKgyC5K_4O0d"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wITFwqj7dSnI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gEzi_4UYhwI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mS6GwSJBaYsX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s_57mncaYsX"
   },
   "source": [
    "## Validate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_2Wg7rXUAhK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJfcFG2SUAhK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
